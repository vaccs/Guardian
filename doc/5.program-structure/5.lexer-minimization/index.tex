
\section{Tokenizer Minimization}
{
	As discussed in the previous section,
	multiple tokenizers are independently created for
	each parser state to enable parser context-sensitive tokenization.
	This feature enables much flexibility in the design of the structure
	of the language to be parsed, but greatly increases the memory demand
	of the parser itself. This is especially bad because many
	of the transitions
	and states of these tokenizers overlap in behavior.
	The tokenizer generator does ensure that it does not generate the
	exact same tokenizer twice, it maintains a cache of all the tokenizers
	it has already generated and checks when invoked whether the requested
	tokenizer has already been created or not.
	This however, does not entirely solve the problem.
	For instance, two tokenizers can be each scanning for 10 tokens, but
	because 1 of those 10 are different, both tokenizers are considered
	entirely different, and all of the states for matching the 9 they have
	in common are duplicated in memory.
	
	The goal of tokenizer minimization is to ``minimize'' the number of
	states needed for all of the tokenizers. This is done by running
	the DFA simplification algorithm on all of the tokenizers of all of the
	parser contexts' state machines together,
	causing any redundant states to be combined. This would result
	in the same behavior as before, but with significantly less memory demand.
	Running the DFA simplification algorithm on such a large number
	of state machines can take a long time, so this pass is disabled by
	default.
}







